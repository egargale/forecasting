2025-09-05  11:48:58.920 | error | uz88v55mwounc9 | worker exited with exit code 1
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 | RuntimeError: Error building extension 'slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0'\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |     raise RuntimeError(message) from e\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |   File "/opt/venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py", line 2612, in _run_ninja_build\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |     _run_ninja_build(\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |   File "/opt/venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py", line 2290, in _write_ninja_file_and_build_library\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |     _write_ninja_file_and_build_library(\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |   File "/opt/venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py", line 2138, in _jit_compile\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |            ^^^^^^^^^^^^^\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |     return _jit_compile(\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |   File "/opt/venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py", line 1681, in load\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |     mod = _load(name + suffix, sources, **myargs)\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |   File "/opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/src/cuda_init.py", line 115, in load\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |                             ^^^^^\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |     cls.mod[repr(config)] = load(\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |   File "/opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/cell.py", line 515, in instance\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |     slstm_cuda = sLSTMCellCUDA.instance(config=config)\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |   File "/opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/cell.py", line 536, in sLSTMCellFuncGenerator\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |     self.func = sLSTMCellFuncGenerator(self.training, config)\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |   File "/opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/cell.py", line 690, in __init__\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |     return sLSTMCell_cuda(config, skip_backend_init=skip_backend_init)\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |   File "/opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/cell.py", line 780, in __new__\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |                       ^^^^^^^^^^^^^^^^^^^^^^\n
2025-09-05  11:48:53.519 | info | uz88v55mwounc9 |     self.slstm_cell = sLSTMCell(self.config)\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |   File "/opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/layer.py", line 78, in __init__\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |            ^^^^^^^^^^^\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |     return sLSTMLayer(\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |   File "/app/tirex/src/tirex/models/mixed_stack.py", line 21, in init_cell\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |     self.slstm_layer = init_cell(config, block_idx, num_blocks)\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |   File "/app/tirex/src/tirex/models/mixed_stack.py", line 58, in __init__\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |     sLSTMBlock(config, block_idx=i, num_blocks=config.num_blocks) if t == "s" else mLSTMBlock(config)\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |   File "/app/tirex/src/tirex/models/mixed_stack.py", line 106, in __init__\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |     model = xLSTMMixedLargeBlockStack(config)\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |   File "/app/tirex/src/tirex/models/tirex.py", line 73, in init_block\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |     self.block_stack, resolved_config = self.init_block(self.model_config.block_kwargs)\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |   File "/app/tirex/src/tirex/models/tirex.py", line 39, in __init__\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |                                                               ^^^^^^^^^^^^^^^^^^\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |     obj = instantiator(cls, _cls_kwargs) if instantiator else cls(**_cls_kwargs)\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |   File "/opt/venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py", line 165, in _load_state\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |     model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |   File "/opt/venv/lib/python3.12/site-packages/lightning/pytorch/core/saving.py", line 91, in _load_from_checkpoint\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |              ^^^^^^^^^^^^^^^^^^^^^^\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |     loaded = _load_from_checkpoint(\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |   File "/opt/venv/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1662, in load_from_checkpoint\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |     return self.method(cls, *args, **kwargs)\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |   File "/opt/venv/lib/python3.12/site-packages/lightning/pytorch/utilities/model_helpers.py", line 125, in wrapper\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |     model = cls.load_from_checkpoint(checkpoint_path, map_location=device, **ckp_kwargs)\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |   File "/app/tirex/src/tirex/base.py", line 37, in from_pretrained\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |     return model_cls.from_pretrained(path, device=device, hf_kwargs=hf_kwargs, ckp_kwargs=ckp_kwargs)\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |   File "/app/tirex/src/tirex/base.py", line 73, in load_model\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |     tirex_model = load_tirex_model("NX-AI/TiRex", device=device_str)\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |   File "/app/rp_handler.py", line 26, in <module>\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 | Traceback (most recent call last):\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 | \n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 | The above exception was the direct cause of the following exception:\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 | \n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 | subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |     raise CalledProcessError(retcode, process.args,\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |   File "/usr/lib/python3.12/subprocess.py", line 571, in run\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |     subprocess.run(\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 |   File "/opt/venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py", line 2595, in _run_ninja_build\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 | Traceback (most recent call last):\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 | ninja: build stopped: subcommand failed.\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 | collect2: error: ld returned 1 exit status\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 | /usr/bin/ld: final link failed: bad value\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 | /usr/bin/ld: slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0.so: hidden symbol `_ZN5slstm21SLSTMPointwiseForwardILb0EEEviiiPK13__nv_bfloat16S3_PKfS3_jPS1_jS6_S6_' isn't defined\n
2025-09-05  11:48:53.518 | info | uz88v55mwounc9 | /usr/bin/ld: tmpxft_000000e7_00000000-6_slstm_forward.compute_80.cudafe1.cpp:(.text+0x4b1): undefined reference to `void slstm::SLSTMPointwiseForward<true>(int, int, int, __nv_bfloat16 const*, __nv_bfloat16 const*, float const*, __nv_bfloat16 const*, unsigned int, __nv_bfloat16*, unsigned int, __nv_bfloat16*, __nv_bfloat16*)'\n
2025-09-05  11:48:53.517 | info | uz88v55mwounc9 | tmpxft_000000e7_00000000-6_slstm_forward.compute_80.cudafe1.cpp:(.text+0x3a8): undefined reference to `void slstm::SLSTMPointwiseForward<false>(int, int, int, __nv_bfloat16 const*, __nv_bfloat16 const*, float const*, __nv_bfloat16 const*, unsigned int, __nv_bfloat16*, unsigned int, __nv_bfloat16*, __nv_bfloat16*)'\n
2025-09-05  11:48:53.517 | info | uz88v55mwounc9 | /usr/bin/ld: slstm_forward.cuda.o: in function `slstm::ForwardPass::IterateInternal(__nv_bfloat16 const*, __nv_bfloat16 const*, float const*, __nv_bfloat16 const*, unsigned int, __nv_bfloat16*, unsigned int, __nv_bfloat16*, __nv_bfloat16*, __nv_bfloat16*)':\n
2025-09-05  11:48:53.517 | info | uz88v55mwounc9 | c++ slstm.o slstm_forward.cuda.o slstm_backward.cuda.o slstm_backward_cut.cuda.o slstm_pointwise.cuda.o blas.cuda.o cuda_error.cuda.o -shared -L/usr/local/cuda/lib -lcublas -L/opt/venv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0.so\n
2025-09-05  11:48:53.517 | info | uz88v55mwounc9 | FAILED: [code=1] slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0.so \n
2025-09-05  11:48:52.387 | info | uz88v55mwounc9 | [8/8] c++ slstm.o slstm_forward.cuda.o slstm_backward.cuda.o slstm_backward_cut.cuda.o slstm_pointwise.cuda.o blas.cuda.o cuda_error.cuda.o -shared -L/usr/local/cuda/lib -lcublas -L/opt/venv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0.so\n
2025-09-05  11:48:52.027 | info | uz88v55mwounc9 | [7/8] c++ -MMD -MF slstm.o.d -DTORCH_EXTENSION_NAME=slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1018\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -fPIC -std=c++17 -DSLSTM_HIDDEN_SIZE=512 -DSLSTM_BATCH_SIZE=8 -DSLSTM_NUM_HEADS=4 -DSLSTM_NUM_STATES=4 -DSLSTM_DTYPE_B=float -DSLSTM_DTYPE_R=__nv_bfloat16 -DSLSTM_DTYPE_W=__nv_bfloat16 -DSLSTM_DTYPE_G=__nv_bfloat16 -DSLSTM_DTYPE_S=__nv_bfloat16 -DSLSTM_DTYPE_A=float -DSLSTM_NUM_GATES=4 -DSLSTM_SIMPLE_AGG=true -DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false -DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0 -DSLSTM_FORWARD_CLIPVAL_VALID=false -DSLSTM_FORWARD_CLIPVAL=0.0 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -c /opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/src/cuda/slstm.cc -o slstm.o \n
2025-09-05  11:48:52.027 | info | uz88v55mwounc9 | ptxas info    : Compile time = 0.852 ms\n
2025-09-05  11:48:52.027 | info | uz88v55mwounc9 | ptxas info    : Used 8 registers, used 0 barriers, 376 bytes cmem[0]\n
2025-09-05  11:48:52.027 | info | uz88v55mwounc9 |     0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 | ptxas info    : Function properties for _Z10initKernelIdEvPT_iS0_\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 | ptxas info    : Compiling entry function '_Z10initKernelIdEvPT_iS0_' for 'sm_86'\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 | ptxas info    : Compile time = 0.839 ms\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 | ptxas info    : Used 8 registers, used 0 barriers, 368 bytes cmem[0]\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 |     0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 | ptxas info    : Function properties for _Z10initKernelIfEvPT_iS0_\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 | ptxas info    : Compiling entry function '_Z10initKernelIfEvPT_iS0_' for 'sm_86'\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 | ptxas info    : Compile time = 1.105 ms\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 | ptxas info    : Used 8 registers, used 0 barriers, 366 bytes cmem[0]\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 |     0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 | ptxas info    : Function properties for _Z10initKernelI6__halfEvPT_iS1_\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 | ptxas info    : Compiling entry function '_Z10initKernelI6__halfEvPT_iS1_' for 'sm_86'\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 | ptxas info    : Compile time = 1.792 ms\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 | ptxas info    : Used 8 registers, used 0 barriers, 366 bytes cmem[0]\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 |     0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 | ptxas info    : Function properties for _Z10initKernelI13__nv_bfloat16EvPT_iS1_\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 | ptxas info    : Compiling entry function '_Z10initKernelI13__nv_bfloat16EvPT_iS1_' for 'sm_86'\n
2025-09-05  11:48:52.026 | info | uz88v55mwounc9 | ptxas info    : 0 bytes gmem\n
2025-09-05  11:48:40.539 | info | uz88v55mwounc9 | [6/8] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output blas.cuda.o.d -DTORCH_EXTENSION_NAME=slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1018\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -Xptxas="-v" -gencode arch=compute_80,code=compute_80 -res-usage --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DSLSTM_HIDDEN_SIZE=512 -DSLSTM_BATCH_SIZE=8 -DSLSTM_NUM_HEADS=4 -DSLSTM_NUM_STATES=4 -DSLSTM_DTYPE_B=float -DSLSTM_DTYPE_R=__nv_bfloat16 -DSLSTM_DTYPE_W=__nv_bfloat16 -DSLSTM_DTYPE_G=__nv_bfloat16 -DSLSTM_DTYPE_S=__nv_bfloat16 -DSLSTM_DTYPE_A=float -DSLSTM_NUM_GATES=4 -DSLSTM_SIMPLE_AGG=true -DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false -DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0 -DSLSTM_FORWARD_CLIPVAL_VALID=false -DSLSTM_FORWARD_CLIPVAL=0.0 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -std=c++17 -c /opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/src/util/blas.cu -o blas.cuda.o \n
2025-09-05  11:48:40.539 | info | uz88v55mwounc9 | ptxas info    : Compile time = 12.694 ms\n
2025-09-05  11:48:40.539 | info | uz88v55mwounc9 | ptxas info    : Used 40 registers, used 0 barriers, 400 bytes cmem[0]\n
2025-09-05  11:48:40.539 | info | uz88v55mwounc9 |     0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n
2025-09-05  11:48:40.539 | info | uz88v55mwounc9 | ptxas info    : Function properties for _ZN50_GLOBAL__N__87594eea_17_slstm_backward_cu_3639fa5e29gradientBiasAggregationKernelEjjjjjjPK13__nv_bfloat16S2_Pf\n
2025-09-05  11:48:40.539 | info | uz88v55mwounc9 | ptxas info    : Compiling entry function '_ZN50_GLOBAL__N__87594eea_17_slstm_backward_cu_3639fa5e29gradientBiasAggregationKernelEjjjjjjPK13__nv_bfloat16S2_Pf' for 'sm_86'\n
2025-09-05  11:48:40.539 | info | uz88v55mwounc9 | ptxas info    : 0 bytes gmem\n
2025-09-05  11:48:40.539 | info | uz88v55mwounc9 | [5/8] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output slstm_backward.cuda.o.d -DTORCH_EXTENSION_NAME=slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1018\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -Xptxas="-v" -gencode arch=compute_80,code=compute_80 -res-usage --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DSLSTM_HIDDEN_SIZE=512 -DSLSTM_BATCH_SIZE=8 -DSLSTM_NUM_HEADS=4 -DSLSTM_NUM_STATES=4 -DSLSTM_DTYPE_B=float -DSLSTM_DTYPE_R=__nv_bfloat16 -DSLSTM_DTYPE_W=__nv_bfloat16 -DSLSTM_DTYPE_G=__nv_bfloat16 -DSLSTM_DTYPE_S=__nv_bfloat16 -DSLSTM_DTYPE_A=float -DSLSTM_NUM_GATES=4 -DSLSTM_SIMPLE_AGG=true -DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false -DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0 -DSLSTM_FORWARD_CLIPVAL_VALID=false -DSLSTM_FORWARD_CLIPVAL=0.0 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -std=c++17 -c /opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/src/cuda/slstm_backward.cu -o slstm_backward.cuda.o \n
2025-09-05  11:48:40.539 | info | uz88v55mwounc9 | ptxas info    : Compile time = 12.791 ms\n
2025-09-05  11:48:40.539 | info | uz88v55mwounc9 | ptxas info    : Used 40 registers, used 0 barriers, 400 bytes cmem[0]\n
2025-09-05  11:48:40.539 | info | uz88v55mwounc9 |     0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n
2025-09-05  11:48:40.539 | info | uz88v55mwounc9 | ptxas info    : Function properties for _ZN54_GLOBAL__N__0809147d_21_slstm_backward_cut_cu_1c2c4d0129gradientBiasAggregationKernelEjjjjjjPK13__nv_bfloat16S2_Pf\n
2025-09-05  11:48:40.539 | info | uz88v55mwounc9 | ptxas info    : Compiling entry function '_ZN54_GLOBAL__N__0809147d_21_slstm_backward_cut_cu_1c2c4d0129gradientBiasAggregationKernelEjjjjjjPK13__nv_bfloat16S2_Pf' for 'sm_86'\n
2025-09-05  11:48:40.539 | info | uz88v55mwounc9 | ptxas info    : 0 bytes gmem\n
2025-09-05  11:48:40.262 | info | uz88v55mwounc9 | [4/8] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output slstm_backward_cut.cuda.o.d -DTORCH_EXTENSION_NAME=slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1018\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -Xptxas="-v" -gencode arch=compute_80,code=compute_80 -res-usage --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DSLSTM_HIDDEN_SIZE=512 -DSLSTM_BATCH_SIZE=8 -DSLSTM_NUM_HEADS=4 -DSLSTM_NUM_STATES=4 -DSLSTM_DTYPE_B=float -DSLSTM_DTYPE_R=__nv_bfloat16 -DSLSTM_DTYPE_W=__nv_bfloat16 -DSLSTM_DTYPE_G=__nv_bfloat16 -DSLSTM_DTYPE_S=__nv_bfloat16 -DSLSTM_DTYPE_A=float -DSLSTM_NUM_GATES=4 -DSLSTM_SIMPLE_AGG=true -DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false -DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0 -DSLSTM_FORWARD_CLIPVAL_VALID=false -DSLSTM_FORWARD_CLIPVAL=0.0 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -std=c++17 -c /opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/src/cuda/slstm_backward_cut.cu -o slstm_backward_cut.cuda.o \n
2025-09-05  11:48:40.262 | info | uz88v55mwounc9 | ptxas info    : Compile time = 6.121 ms\n
2025-09-05  11:48:40.262 | info | uz88v55mwounc9 | ptxas info    : Used 38 registers, used 0 barriers, 480 bytes cmem[0]\n
2025-09-05  11:48:40.262 | info | uz88v55mwounc9 |     0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n
2025-09-05  11:48:40.262 | info | uz88v55mwounc9 | ptxas info    : Function properties for _ZN5slstm22SLSTMPointwiseBackwardEiiiPK13__nv_bfloat16jS2_S2_PKfS2_jS2_jPS0_jS5_S5_S5_\n
2025-09-05  11:48:40.261 | info | uz88v55mwounc9 | ptxas info    : Compiling entry function '_ZN5slstm22SLSTMPointwiseBackwardEiiiPK13__nv_bfloat16jS2_S2_PKfS2_jS2_jPS0_jS5_S5_S5_' for 'sm_86'\n
2025-09-05  11:48:40.261 | info | uz88v55mwounc9 | ptxas info    : Compile time = 7.589 ms\n
2025-09-05  11:48:40.261 | info | uz88v55mwounc9 | ptxas info    : Used 36 registers, used 0 barriers, 440 bytes cmem[0]\n
2025-09-05  11:48:40.261 | info | uz88v55mwounc9 |     0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n
2025-09-05  11:48:40.261 | info | uz88v55mwounc9 | ptxas info    : Function properties for _ZN5slstm21SLSTMPointwiseForwardILb1EEEviiiPK13__nv_bfloat16S3_PKfS3_jPS1_jS6_S6_\n
2025-09-05  11:48:40.261 | info | uz88v55mwounc9 | ptxas info    : Compiling entry function '_ZN5slstm21SLSTMPointwiseForwardILb1EEEviiiPK13__nv_bfloat16S3_PKfS3_jPS1_jS6_S6_' for 'sm_86'\n
2025-09-05  11:48:40.261 | info | uz88v55mwounc9 | ptxas info    : Compile time = 7.676 ms\n
2025-09-05  11:48:40.261 | info | uz88v55mwounc9 | ptxas info    : Used 38 registers, used 0 barriers, 440 bytes cmem[0]\n
2025-09-05  11:48:40.261 | info | uz88v55mwounc9 |     0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n
2025-09-05  11:48:40.261 | info | uz88v55mwounc9 | ptxas info    : Function properties for _ZN5slstm21SLSTMPointwiseForwardILb0EEEviiiPK13__nv_bfloat16S3_PKfS3_jPS1_jS6_S6_\n
2025-09-05  11:48:40.261 | info | uz88v55mwounc9 | ptxas info    : Compiling entry function '_ZN5slstm21SLSTMPointwiseForwardILb0EEEviiiPK13__nv_bfloat16S3_PKfS3_jPS1_jS6_S6_' for 'sm_86'\n
2025-09-05  11:48:40.261 | info | uz88v55mwounc9 | ptxas info    : 0 bytes gmem\n
2025-09-05  11:48:40.024 | info | uz88v55mwounc9 | [3/8] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output slstm_pointwise.cuda.o.d -DTORCH_EXTENSION_NAME=slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1018\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -Xptxas="-v" -gencode arch=compute_80,code=compute_80 -res-usage --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DSLSTM_HIDDEN_SIZE=512 -DSLSTM_BATCH_SIZE=8 -DSLSTM_NUM_HEADS=4 -DSLSTM_NUM_STATES=4 -DSLSTM_DTYPE_B=float -DSLSTM_DTYPE_R=__nv_bfloat16 -DSLSTM_DTYPE_W=__nv_bfloat16 -DSLSTM_DTYPE_G=__nv_bfloat16 -DSLSTM_DTYPE_S=__nv_bfloat16 -DSLSTM_DTYPE_A=float -DSLSTM_NUM_GATES=4 -DSLSTM_SIMPLE_AGG=true -DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false -DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0 -DSLSTM_FORWARD_CLIPVAL_VALID=false -DSLSTM_FORWARD_CLIPVAL=0.0 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -std=c++17 -c /opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/src/cuda/slstm_pointwise.cu -o slstm_pointwise.cuda.o \n
2025-09-05  11:48:40.024 | info | uz88v55mwounc9 | \n
2025-09-05  11:48:40.024 | info | uz88v55mwounc9 | /opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/src/cuda/slstm_pointwise.cuh(11): warning #20280-D: when "-static-global-template-stub=true" in whole program compilation mode ("-rdc=false"), a __global__ function template instantiation or specialization ("slstm::SLSTMPointwiseForward<(bool)0> ") must have a definition in the current translation unit. To resolve this issue, either use separate compilation mode ("-rdc=true"), or explicitly set "-static-global-template-stub=false" (but see nvcc documentation about downsides of turning it off)\n
2025-09-05  11:48:40.024 | info | uz88v55mwounc9 | \n
2025-09-05  11:48:40.024 | info | uz88v55mwounc9 | Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"\n
2025-09-05  11:48:40.024 | info | uz88v55mwounc9 | \n
2025-09-05  11:48:40.024 | info | uz88v55mwounc9 | /opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/src/cuda/slstm_pointwise.cuh(11): warning #20280-D: when "-static-global-template-stub=true" in whole program compilation mode ("-rdc=false"), a __global__ function template instantiation or specialization ("slstm::SLSTMPointwiseForward<(bool)1> ") must have a definition in the current translation unit. To resolve this issue, either use separate compilation mode ("-rdc=true"), or explicitly set "-static-global-template-stub=false" (but see nvcc documentation about downsides of turning it off)\n
2025-09-05  11:48:40.024 | info | uz88v55mwounc9 | ptxas info    : 0 bytes gmem\n
2025-09-05  11:48:40.024 | info | uz88v55mwounc9 | \n
2025-09-05  11:48:40.024 | info | uz88v55mwounc9 | /opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/src/cuda/slstm_pointwise.cuh(11): warning #20280-D: when "-static-global-template-stub=true" in whole program compilation mode ("-rdc=false"), a __global__ function template instantiation or specialization ("slstm::SLSTMPointwiseForward<(bool)0> ") must have a definition in the current translation unit. To resolve this issue, either use separate compilation mode ("-rdc=true"), or explicitly set "-static-global-template-stub=false" (but see nvcc documentation about downsides of turning it off)\n
2025-09-05  11:48:40.024 | info | uz88v55mwounc9 | \n
2025-09-05  11:48:40.024 | info | uz88v55mwounc9 | Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"\n
2025-09-05  11:48:40.024 | info | uz88v55mwounc9 | \n
2025-09-05  11:48:40.024 | info | uz88v55mwounc9 | /opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/src/cuda/slstm_pointwise.cuh(11): warning #20280-D: when "-static-global-template-stub=true" in whole program compilation mode ("-rdc=false"), a __global__ function template instantiation or specialization ("slstm::SLSTMPointwiseForward<(bool)1> ") must have a definition in the current translation unit. To resolve this issue, either use separate compilation mode ("-rdc=true"), or explicitly set "-static-global-template-stub=false" (but see nvcc documentation about downsides of turning it off)\n
2025-09-05  11:48:39.899 | info | uz88v55mwounc9 | [2/8] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output slstm_forward.cuda.o.d -DTORCH_EXTENSION_NAME=slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1018\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -Xptxas="-v" -gencode arch=compute_80,code=compute_80 -res-usage --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DSLSTM_HIDDEN_SIZE=512 -DSLSTM_BATCH_SIZE=8 -DSLSTM_NUM_HEADS=4 -DSLSTM_NUM_STATES=4 -DSLSTM_DTYPE_B=float -DSLSTM_DTYPE_R=__nv_bfloat16 -DSLSTM_DTYPE_W=__nv_bfloat16 -DSLSTM_DTYPE_G=__nv_bfloat16 -DSLSTM_DTYPE_S=__nv_bfloat16 -DSLSTM_DTYPE_A=float -DSLSTM_NUM_GATES=4 -DSLSTM_SIMPLE_AGG=true -DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false -DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0 -DSLSTM_FORWARD_CLIPVAL_VALID=false -DSLSTM_FORWARD_CLIPVAL=0.0 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -std=c++17 -c /opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/src/cuda/slstm_forward.cu -o slstm_forward.cuda.o \n
2025-09-05  11:48:39.899 | info | uz88v55mwounc9 | ptxas info    : 0 bytes gmem\n
2025-09-05  11:48:38.840 | info | uz88v55mwounc9 | [1/8] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda_error.cuda.o.d -DTORCH_EXTENSION_NAME=slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1018\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -Xptxas="-v" -gencode arch=compute_80,code=compute_80 -res-usage --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DSLSTM_HIDDEN_SIZE=512 -DSLSTM_BATCH_SIZE=8 -DSLSTM_NUM_HEADS=4 -DSLSTM_NUM_STATES=4 -DSLSTM_DTYPE_B=float -DSLSTM_DTYPE_R=__nv_bfloat16 -DSLSTM_DTYPE_W=__nv_bfloat16 -DSLSTM_DTYPE_G=__nv_bfloat16 -DSLSTM_DTYPE_S=__nv_bfloat16 -DSLSTM_DTYPE_A=float -DSLSTM_NUM_GATES=4 -DSLSTM_SIMPLE_AGG=true -DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false -DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0 -DSLSTM_FORWARD_CLIPVAL_VALID=false -DSLSTM_FORWARD_CLIPVAL=0.0 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -std=c++17 -c /opt/venv/lib/python3.12/site-packages/xlstm/blocks/slstm/src/util/cuda_error.cu -o cuda_error.cuda.o \n
2025-09-05  11:48:38.840 | info | uz88v55mwounc9 | components.py       :146  2025-09-05 09:48:37,543 {'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/usr/local/cuda/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=512', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas="-v"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=512', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n
2025-09-05  11:48:38.839 | info | uz88v55mwounc9 | W0905 09:48:37.542000 19 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n
2025-09-05  11:48:38.839 | info | uz88v55mwounc9 | W0905 09:48:37.542000 19 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n
2025-09-05  11:48:37.537 | info | uz88v55mwounc9 | cuda_init.py        :114  2025-09-05 09:48:37,536 Before compilation and loading of slstm.\n
2025-09-05  11:48:37.537 | info | uz88v55mwounc9 | Loading models...\n
2025-09-05  11:48:37.537 | info | uz88v55mwounc9 | CUDA mode detected - using GPU acceleration\n
2025-09-05  11:48:37.537 | info | uz88v55mwounc9 | font_manager.py     :1639 2025-09-05 09:48:21,418 generated new fontManager\n